$1

Quick API List & How They Work (TL;DR)
â€¢ GET /health â€” Liveness check.
â€¢ How it works: Pings the API process. Used by load balancers/monitors.
â€¢ GET /tools â€” Available analyzers + versions.
â€¢ How it works: Reads the installed/allowed tools that the orchestrator can invoke.
â€¢ POST /analyze â€” Submit a scan (sync or auto-async based on size/time).
â€¢ Inputs: github_url or file (ZIP), optional ref , commit , include , exclude ,
analyzers , timeout_sec , labels .
â€¢ Output: 200 { job_id, summary } (small jobs) or 202 { job_id, status: "running" } .
â€¢ How it works: Creates job_id â†’ clones/extracts into storage/workspace/{job_id} â†’ selects
analyzers

â†’

runs

them

in

â†’

parallel

findings

â†’

writes

background

task;

emits

merges

storage/reports/{job_id}.json â†’ returns summary.
â€¢ POST /analyze-async â€” Always queue the job and return immediately.
â€¢ Output: 202 { job_id, status: "queued" } .
â€¢ How

it

works:

Same

pipeline

as

/analyze ,

but

always

via

report.created on completion.
â€¢ GET /jobs/{job_id} â€” Check job status & progress.
â€¢ How it works: Reads job state from storage/logs/{job_id}.json /in-memory registry; phases
include clone , analyze:<tool> , merge , write .
â€¢ DELETE /jobs/{job_id} â€” Cancel a running/queued job.
â€¢ How it works: Signals the worker to stop; orchestrator stops scheduling new tool runs and cleans
workspace.
â€¢ POST /jobs/{job_id}/rerun â€” Re-run the same inputs.
â€¢ How it works: Re-enqueues job with same source and analyzer selection; produces a new report.
â€¢ GET /reports/{job_id} â€” Fetch the full normalized report.

1

â€¢ How it works: Serves

storage/reports/{job_id}.json

(contains

summary ,

files[] ,

meta ).
â€¢ GET /reports â€” List/paginate reports with filters.
â€¢ How it works: Scans report metadata and returns a paginated list; supports severity , tool ,
repo , since/until , label filters.
â€¢ GET /reports/{job_id}/summary â€” Lightweight counts only.
â€¢ How it works: Reads summary section only (cheaper for dashboards).
â€¢ GET /events/{job_id} (SSE) â€” Live progress stream.
â€¢ How it works: Streams orchestrator progress events ( progress , finished ) to clients during
scans.
â€¢ POST /webhooks/register â€” Register a webhook for report.created .
â€¢ How it works: Stores URL + secret; on completion, POSTs payload with HMAC signature; retries on
5xx/timeouts.
â€¢ DELETE /webhooks/{id} â€” Remove a webhook.
â€¢ How it works: Deletes registration; no further deliveries.
â€¢ GET /config/analyzers â€” Read defaults/rulesets/allow-list.
â€¢ How it works: Returns current analyzer configuration the orchestrator uses for selection.
â€¢ PATCH /config/analyzers â€” Update analyzer defaults/rulesets/allow-list.
â€¢ How it works: Persists config (e.g., semgrep rulesets, URL allow-list) used by future jobs.
Where each call fits: /analyze* starts a scan â†’ /jobs/* monitors/controls it â†’ /
reports* fetches results â†’ /events & webhooks notify progress/results â†’ /config/*
tunes behavior â†’ /tools advertises capabilities â†’ /health is for ops.
This document is the authoritative reference for the CodeAgent Vulnerability Scanner service. It covers how
the API works endâ€‘toâ€‘end, all endpoints and models, job lifecycle, rate limits, auth, error codes, webhooks/
SSE, analyzer configuration, and implementation notes. It is designed so you can hand it to frontend devs,
partner integrators, or your future self.

2

1) Overview & Architecture
What it does: Accepts a GitHub repo URL or a ZIP upload â†’ clones/extracts â†’ runs analyzers (Semgrep,
Bandit, dependency audit) â†’ produces a normalized JSON report grouped by file and severity â†’ exposes it
via /reports/{job_id} . Optional async mode, SSE progress, and webhooks. Emits an internal
report.created event for the agentic layer (autofix/test/PR creation).
Components - FastAPI app ( /analyze , /reports , /jobs , /events , /webhooks , /config ). Ingestion: safe clone/extract into storage/workspace/{job_id} with ignores. - Analyzers: plugin
runners (Semgrep, Bandit, pipâ€‘audit), concurrent execution with timeouts. - Orchestrator: merges findings
â†’ normalized report â†’ writes to storage/reports/{job_id}.json . - Events: optional webhooks
(HTTP POST) and inâ€‘process bus for agentic subscribers.

2) Authentication, Headers, Formats
2.1 Auth
â€¢ Recommended (prod): Authorization: Bearer <API_KEY> (HMAC/DB check serverâ€‘side)
â€¢ Dev: unauthenticated

2.2 Global Headers
â€¢ Idempotency-Key: <uuid> â€” prevents duplicate scans on retries
â€¢ X-Client-Id: <string> â€” (optional) tenant/tracking
â€¢ X-Request-Id â€” returned on each response; you can also supply one

2.3 Content Types
â€¢ Requests: multipart/form-data (uploads), application/json (control/config)
â€¢ Responses: application/json (except SSE stream)

2.4 Versions
â€¢ Accept: application/vnd.codeagent.v1+json (optional vendor versioning)
â€¢ Minor, backwardâ€‘compatible changes bump version field in /health and OpenAPI

3) Limits, Quotas, Timeouts
â€¢ Upload cap: 50 MB (413 on exceed)
â€¢ Per job limits: max files (e.g., 10,000), perâ€‘tool timeout (default 600s), overall wallâ€‘clock (e.g., 900s)
â€¢ Rate limit: 60 req/min per API key; max 2 concurrent running jobs per key
â€¢ Ignore lists: .git , node_modules , venv , .venv , dist , build , __pycache__ , large
binaries > 20MB
â€¢ URL allowâ€‘list: https://github.com/* initially; can be extended via config

3

4) Data Model
4.1 Severity Enum
critical | high | medium | low

4.2 Issue (normalized finding)
{
"tool": "bandit",
"type": "B608",
"message": "Possible SQL injection via string building",
"severity": "high",
"file": "src/app.py",
"line": 42,
"rule_id": "B608",
"suggestion": "Use parameterized queries"
}

4.3 FileIssues
{ "path": "src/app.py", "issues": [Issue, ...] }

4.4 Report
{
"job_id": "4b3a...",
"meta": {
"tools": ["semgrep","bandit","pip-audit"],
"repo": {"source": "github|zip", "url": "https://github.com/org/repo",
"ref": "main", "commit": null},
"generated_at": "2025-10-17T14:30:00Z",
"duration_ms": 23456,
"labels": ["customer:acme","case:123"]
},
"summary": {"critical": 1, "high": 3, "medium": 7, "low": 4},
"files": [FileIssues, ...]
}

4

4.5 Error
{
"error": {
"code": "INVALID_INPUT|NOT_FOUND|TIMEOUT|INTERNAL|RATE_LIMIT|
PAYLOAD_TOO_LARGE|UNAUTHORIZED|FORBIDDEN",
"message": "Human-readable",
"details": {"field": "github_url"}
}
}

5) Job Lifecycle
queued â†’ running (clone â†’ analyze:N tools â†’ merge â†’ write) â†’ completed | failed |
canceled | expired
â€¢ Creation: /analyze or /analyze-async returns job_id
â€¢ Progress: /jobs/{job_id} or SSE /events/{job_id}
â€¢ Result: /reports/{job_id} once completed
â€¢ Cancel: DELETE /jobs/{job_id} when queued|running

6) Endpoints
6.1 Health & Metadata
GET /health â†’ 200 { "status": "ok", "version": "0.1.0" }
GET

/tools

â†’

200

{

"available":

["bandit","semgrep","dep"],

"default":

["bandit","semgrep","dep"], "versions": {"bandit":"1.7.9","semgrep":"1.81.0"} }

6.2 Submit Scan (Sync / Autoâ€‘Async)
POST /analyze â€” may reply 200 (small jobs) or 202 (accepted) based on size/timeout hints.
Headers: Content-Type: multipart/form-data , optional Idempotency-Key
Form fields - github_url (string, optional) â€” https://github.com/org/repo - ref (string, optional)
â€” branch/tag - commit (string, optional) â€” SHA to checkout - file (file, optional) â€” ZIP archive include (string, optional) â€” comma CSV of globs ( src/**/*.py ) - exclude (string, optional) â€” comma
CSV of globs ( tests/**,docs/** ) - analyzers (string, optional) â€” CSV: bandit,semgrep,dep -

5

timeout_sec

(int, optional) â€” overrides default -

labels

(string, optional) â€” CSV labels

( customer:acme,case:123 )
Responses - 200 OK â€” { "job_id": "...", "summary": { ... } } - 202 Accepted â€”
{ "job_id": "...", "status": "running" } - 400/401/403/413/429/500 â€” Error
Validation rules - Require either github_url or file (not both) - Validate github_url host and
scheme; enforce allowâ€‘list - Enforce upload size cap and filename checks

6.3 Submit Scan (Async)
POST /analyze-async
Request â€” same fields as /analyze .
202 Accepted

{ "job_id": "uuid", "status": "queued" }

Background task performs clone/extract â†’ analyze â†’ write report â†’ emit report.created and deliver
webhooks.

6.4 Jobs
GET /jobs/{job_id} â†’ status & timestamps

{
"job_id": "...",
"status": "queued|running|completed|failed|canceled|expired",
"progress": {"phase": "clone|analyze:semgrep|analyze:bandit|merge|write",
"percent": 65},
"submitted_at": "...",
"started_at": "...",
"finished_at": "...",
"error": null
}

DELETE /jobs/{job_id} â€” cancel - 202 { "job_id": "...", "status": "canceling" } - 409
if not cancellable

6

POST /jobs/{job_id}/rerun â€” requeue with same inputs - 202 { "job_id": "...", "status":
"queued" }

6.5 Reports
GET /reports/{job_id} â€” full report - 200 Report - 404 Not found (not finished/doesnâ€™t exist)
GET

/reports

â€” paginate/filter Query:

page ,

limit

(â‰¤100),

severity=high,critical ,

tool=semgrep , repo=https://github.com/org/repo , since , until , label

{
"items": [Report, ...],
"page": 1,
"limit": 20,
"total": 57
}

GET /reports/{job_id}/summary â€” light summary

{ "job_id": "...", "summary": {"critical":0,"high":2,"medium":7,"low":5} }

6.6 Live Progress (SSE)
GET /events/{job_id} - Header: Accept: text/event-stream - Server emits events:

:event: progress
:data: {"phase":"semgrep","percent":32}
:event: finished
:data: {"job_id":"...","status":"completed"}

6.7 Webhooks
POST /webhooks/register â€” register a URL to receive report.created

{ "url": "https://example.com/hooks", "events": ["report.created"], "secret":
"optional" }

7

- 201 { "id": "wh_123" }
Delivery - Method:

POST

- Headers:

X-Event: report.created ,

X-Signature: sha256=...

(HMAC over body) - Body:

{ "job_id": "...", "repo": {"url":"..."}, "summary": {...}, "report_url": "/
reports/..." }
- Retries with exponential backoff on 5xx/timeouts; disable after N attempts
DELETE /webhooks/{id} â€” unregister

6.8 Configuration
GET /config/analyzers

{ "defaults": ["bandit","semgrep","dep"], "rulesets": {"semgrep":["p/owasp-topten","p/secrets"], "bandit": []}, "allow_list": ["https://github.com/"] }

PATCH /config/analyzers

{ "defaults": ["semgrep","dep"], "rulesets": {"semgrep":["p/owasp-top-ten","p/
secrets"]}, "allow_list": ["https://github.com/", "https://gitlab.com/"] }
- 200 { "ok": true }

7) How Scanning Works (under the hood)
1. Ingestion
2. Validate inputs â†’ create job_id â†’ make storage/workspace/{job_id}
3. git clone --depth=1 {url} (or extract ZIP)
4. Remove ignored folders; optionally apply include / exclude globs
5. Analyzer selection
6. Parse analyzers CSV or use defaults
7. Enable Bandit only if .py files exist; Dep audit only if requirements.txt exists (JS/npm audit/
OSV later)
8. Execution
9. Run in parallel (ThreadPoolExecutor), each with a perâ€‘tool timeout
10. Capture JSON output (Semgrep/Bandit/pipâ€‘audit); map to normalized Issue
11. Merge

8

12. Compute severity summary; group by file; attach metadata (tools, repo, duration)
13. Persist & notify
14. Write storage/reports/{job_id}.json
15. Update job state to completed or failed
16. Emit report.created (events + webhooks)
Security note: The service never executes repo code. It only runs static analyzers over files.
All paths are confined under the job workspace.

8) Error Handling & Status Codes
â€¢ 200 OK â€” success (sync)
â€¢ 202 Accepted â€” queued/running (async or autoâ€‘async)
â€¢ 400 INVALID_INPUT â€” missing/invalid fields; both github_url and file sent; bad URL host
â€¢ 401 UNAUTHORIZED â€” missing/invalid API key (prod)
â€¢ 403 FORBIDDEN â€” plan/tenant restrictions
â€¢ 404 NOT_FOUND â€” unknown job_id /report
â€¢ 409 CONFLICT â€” cannot cancel/rerun
â€¢ 413 PAYLOAD_TOO_LARGE â€” upload exceeds limit
â€¢ 429 RATE_LIMIT â€” throttled; retry after Retry-After
â€¢ 500 INTERNAL â€” unexpected server error
â€¢ 504 TIMEOUT â€” perâ€‘tool or overall timeout exceeded
Each error uses the Error shape from Â§4.5 and returns an X-Request-Id header.

9) OpenAPI (runnable excerpt)
openapi: 3.0.3
info:
title: CodeAgent Scanner API
version: 0.1.0
servers:
- url: http://localhost:8080
paths:
/health:
get:
summary: Health check
responses:
'200': { description: OK }
/analyze:
post:
summary: Submit a scan (may return 200 or 202)
requestBody:
required: true

9

content:
multipart/form-data:
schema:
type: object
properties:
github_url: { type: string, format: uri }
ref: { type: string }
commit: { type: string }
file: { type: string, format: binary }
include: { type: string }
exclude: { type: string }
analyzers: { type: string }
timeout_sec: { type: integer }
labels: { type: string }
responses:
'200': { description: OK }
'202': { description: Accepted }
'400': { description: Bad Request }
'413': { description: Payload Too Large }
'500': { description: Internal Error }
/reports/{job_id}:
get:
summary: Get full report
parameters:
- in: path
name: job_id
required: true
schema: { type: string }
responses:
'200': { description: OK }
'404': { description: Not Found }
Full OpenAPI can be generated directly by FastAPI from the routers; this excerpt anchors
required fields.

10) Usage Examples
GitHub URL (autoâ€‘async on big repos)
curl -F github_url=https://github.com/pallets/flask
-F analyzers=bandit,semgrep,dep
http://localhost:8080/analyze

10

ZIP Upload (explicit async)
curl -F file=@/path/to/project.zip
-F include="src/**/*.py"
http://localhost:8080/analyze-async

Get Status & Report
curl http://localhost:8080/jobs/<job_id> | jq
curl http://localhost:8080/reports/<job_id> | jq

Register Webhook
curl -X POST http://localhost:8080/webhooks/register
-H 'Content-Type: application/json'
-d '{"url":"https://example.com/hooks","events":
["report.created"],"secret":"xyz"}'

11) Implementation Notes (for engineers)
â€¢ Routers: api/routers/analyze.py implements /analyze , /analyze-async , /jobs/* , /
reports/* , /events/* .
â€¢ BackgroundTasks: used to offload large jobs; swap to Celery/RQ later for retries.
â€¢ ThreadPool: used per job to run analyzers in parallel; cap with env vars.
â€¢ Storage layout:
â€¢ storage/workspace/{job_id} â€” cloned/extracted repo
â€¢ storage/reports/{job_id}.json â€” final report
â€¢ storage/logs/{job_id}.json â€” status/progress, errors
â€¢ Security: never execute repo code; sanitize paths; enforce allowâ€‘lists; set subprocess timeouts.
â€¢ Observability: log JSON with job_id correlation; expose X-Request-Id .

12) Agentic Integration Hooks
â€¢ After report write: emit report.created (internal bus) and deliver webhooks.
â€¢ Subscriber (agent orchestrator) clusters issues â†’ proposes patches/tests â†’ opens PR.
â€¢ Optionally expose /agents/actions in future to attach patch_unified back to a job.

11

13) Retention & Privacy (defaults)
â€¢ Workspace purged after 7 days; reports retained 30 days (configurable)
â€¢ No PII processed unless present in repo; avoid storing access tokens in logs
â€¢ Provide /jobs/{job_id} DELETE to purge workspace & report early

14) FAQ
â€¢ Why 200 vs 202 on /analyze ? Small repos may complete within request timeout â†’ 200 . Larger
ones return 202 and continue in background.
â€¢ Do you execute user code? No. Only static analysis tools run.
â€¢ Can I scan private repos? Yes via deploy token/credential injection (future); start with public repos.
â€¢ How do I tune false positives? Update /config/analyzers rulesets and perâ€‘tool ignore config;
honor .semgrepignore and # nosec pragmas where feasible.

12



ğŸ—ï¸ Application Architecture Overview
The backend follows a layered microservice architecture with these key components:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FastAPI App                           â”‚
â”‚              (HTTP Request Handler)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Job Orchestrator                         â”‚
â”‚           (Background Job Manager)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚           â”‚           â”‚
â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚ Ingestion â”‚ â”‚Anal-â”‚ â”‚   Pipeline  â”‚
â”‚   Layer   â”‚ â”‚yzersâ”‚ â”‚    Layer    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜